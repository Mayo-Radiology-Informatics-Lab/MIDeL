{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Section 13: Tabular Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mayo-Radiology-Informatics-Lab/MIDeL/blob/main/chapters/13A.ipynb)"
      ],
      "metadata": {
        "id": "ghJw2xL8ZPCY"
      },
      "id": "ghJw2xL8ZPCY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Author: Yashbir Singh, Ph.D.*"
      ],
      "metadata": {
        "id": "C1eFGlEUZLo1"
      },
      "id": "C1eFGlEUZLo1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. Introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "6QgJ7B97hq9J"
      },
      "id": "6QgJ7B97hq9J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Deep neural networks have shown great promise in various domains, including images, audio, and text. Several commonly used methods encode raw data into meaningful representations in these domains [1, 2, 3].In most real-world applications, such canonical architectures perform well. However, deep learning approaches have lagged behind traditional methods when applied to tabular data. Tabular data consists of samples (rows) with a consistent set of features (columns) and is the most common data type in real-world applications [1,2]. Tabular data is routinely used in many fields, such as medicine, finance, manufacturing, and climate science. This section discusses state-of-the-art traditional machine learning approaches and recent advances in deep neural models applied to tabular data. Traditional machine learning methods, such as gradient-boosted decision trees (GBDT) [4], dominated tabular data modeling and outperformed deep learning during the previous decade. Despite their theoretical advantages [5, 6, 7], deep neural networks face several challenges when applied to tabular data such as lack of locality, difficulty handling data sparsity (missing values), or mixed feature types (numerical, ordinal, and categorical), and a lack of prior knowledge about the dataset structure (unlike with text or images).Furthermore, deep neural networks are regarded as a \"black box\" approach, in which the way input data is transformed into model outputs is opaque or difficult to understand [5, 7]. Despite the \"no free lunch\" principle [8], tree-ensemble algorithms, such as XGBoost, remain recommended for real-world tabular data problems [1].Several recent studies have applied deep learning to the tabular data domain, introducing new neural architectures to improve tabular data performance, briefly discussed below [10, 9, 13]. Some claim to outperform traditional standard approaches like GBDT. Unfortunately, due to the lack of a standard benchmark (such as ImageNet [10] and GLUE [11]), publications in this field tend to use different datasets, making apples-to-apples comparisons difficult. \n"
      ],
      "metadata": {
        "id": "ZkgVnPFohzbt"
      },
      "id": "ZkgVnPFohzbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Differentiable trees"
      ],
      "metadata": {
        "id": "4rmjTMpiitfW"
      },
      "id": "4rmjTMpiitfW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Traditional decision trees cannot be used as part of end-to-end training pipelines because they are not differentiable and thus do not allow for gradient optimization. Differentiability is a key mathematical property that makes it possible to predict the best direction to change parameters during the process of training a model; this is known as gradient descent optimization and is the primary method for training deep neural networks. Differentiable tree methods seek ways to make decision trees differentiable, because of their excellent performance on tabular data.  These methods generally do this by smoothing the decision functions in the internal tree nodes to make the tree function and tree routing differentiable [14], allowing the advantages of decision trees to be incorporated into deep neural networks.\n",
        "\n"
      ],
      "metadata": {
        "id": "moQxFUkHixWb"
      },
      "id": "moQxFUkHixWb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decision Tree"
      ],
      "metadata": {
        "id": "dd8R3-9ui87u"
      },
      "id": "dd8R3-9ui87u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####A white box machine learning algorithm is a Decision Tree. Internal decision-making logic is shared, which is not available in black box algorithms like Neural Networks. When compared to the neural network algorithm, it takes less time to train. The number of records and attributes in the given data determine the time complexity of decision trees. The decision tree is a non-parametric or distribution-free method that does not rely on probability distribution assumptions. Decision trees can handle high-dimensional data with better accuracy [4, 13].\n",
        "\n"
      ],
      "metadata": {
        "id": "u2As3ZN2jAQ4"
      },
      "id": "u2As3ZN2jAQ4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Attention-based models"
      ],
      "metadata": {
        "id": "aWKbcfE2jJpo"
      },
      "id": "aWKbcfE2jJpo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Due to the popularity of attention-based models in various fields, attention-like modules have been proposed  for deep tabular networks. For example, intra-sample attention, in which features of a given sample interact with other features of the same sample, and inter-sample attention, in which features of one sample interact with features of other samples, introduce attention-like models to tabular data [9, 15].\n",
        "\n"
      ],
      "metadata": {
        "id": "6zOul7QujN5j"
      },
      "id": "6zOul7QujN5j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TabNet"
      ],
      "metadata": {
        "id": "Bw8YJvtRIMgm"
      },
      "id": "Bw8YJvtRIMgm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####TabNet is a deep learning end-to-end model that performs well across various datasets [9]. It has an encoder in which sequential decision steps encode features using sparse learned masks, then use the mask to select relevant features for each row (with attention). The encoder uses sparsemax (a new activation function similar to the traditional softmax) layers to choose a small number of features. Learning masks have the advantage that feature selection does not have to be all-or-nothing. A learnable mask can make a soft decision instead of a hard threshold on a feature, which relaxes traditional (non-differentiable) feature selection methods [9].\n"
      ],
      "metadata": {
        "id": "4L2LBqyHIVud"
      },
      "id": "4L2LBqyHIVud"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neural Oblivious Decision Ensembles (NODE)"
      ],
      "metadata": {
        "id": "m0piTBaWIjcy"
      },
      "id": "m0piTBaWIjcy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Equal-depth decision trees (EDTs) are differentiable in the NODE network [13], allowing error gradients to backpropagate through them. ODTs, like traditional decision trees, split data into categories based on selected features and compare each to a learned threshold. However, at each level, only one feature is chosen, resulting in a balanced and distinguishable ODT. As a result, the entire model generates a set of differentiable trees. This can be considered a special case of the differentiable tree approach previously discussed.\n",
        " \n"
      ],
      "metadata": {
        "id": "-CXXIwuDIsZv"
      },
      "id": "-CXXIwuDIsZv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DNF-Net"
      ],
      "metadata": {
        "id": "ew1gTZMXIxuH"
      },
      "id": "ew1gTZMXIxuH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####DNF-Net is a deep neural network that simulates disjunctive normal formulas (DNF). Unfortunately, like decision trees, such functions are not differentiable. The authors proposed that hard Boolean formulas be replaced with soft, differentiable versions. The disjunctive normal neural form (DNNF) block, which contains (1) a fully connected layer and (2) a DNNF layer formed by a soft version of binary conjunctions over literals, is a key feature of this model. The entire model is made up of these DNNFs [10].\n",
        " \n"
      ],
      "metadata": {
        "id": "j47yBo5bI4TJ"
      },
      "id": "j47yBo5bI4TJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1D-CNN"
      ],
      "metadata": {
        "id": "eStSoGnFI8z4"
      },
      "id": "eStSoGnFI8z4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####In a Kaggle competition with tabular data, a 1D-convolutional neural network (CNN) recently won the best single model performance. The model is based on the idea that the CNN structure is good at extracting features. Even so, because the feature ordering has no locality characteristics, it is rarely used in tabular data. A fully connected layer is used to create a larger set of features with locality characteristics in this model, which is then followed by several 1D-Conv layers with shortcut-like connections [16]. Despite its excellent performance in this Kaggle competition, we suggest caution; this approach is likely not generalizable.\n",
        " \n"
      ],
      "metadata": {
        "id": "3A5oU43UJCF3"
      },
      "id": "3A5oU43UJCF3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implementation Description of Tabular data"
      ],
      "metadata": {
        "id": "d-0unwl_JNxZ"
      },
      "id": "d-0unwl_JNxZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tabular data structures are commonly used to store these two primary components of a study. A table's rows contain observations (such as individuals or events), while the table's columns contain the measurements taken on each individual. The table below, for example, contains responses to questions sent to a group of people. Answers to the questions are provided by each person (individual) (the measurements).\n"
      ],
      "metadata": {
        "id": "pxWMX67hJQyg"
      },
      "id": "pxWMX67hJQyg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical Record Number (MRN) (Patients) | Name | Gender|Age| \n",
        "--- | --- | ---| ---|\n",
        "M234 | A| Male |47|\n",
        "M244 | B| Female|35|\n",
        "M342 | C| Female|38|\n",
        "M567 | D| Male|26|\n",
        "\n",
        "\n",
        "                                                     Table1. Example of a table\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eXyABrpMKW-Y"
      },
      "id": "eXyABrpMKW-Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The structure of a table###"
      ],
      "metadata": {
        "id": "B2gT2BwKIqvn"
      },
      "id": "B2gT2BwKIqvn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tables are two-dimensional data structures that store attributes of samples. Every row of a table shows the set of attributes for a particular sample. A column is made up of data with a uniform type on which statistics are computed. Columns are also known as features or attributes. Table labels identify the rows and columns in the table: (i)The label for each row is also called its index. (ii) The table's column names are referred to as column labels.\n"
      ],
      "metadata": {
        "id": "kizWyMnd1GGn"
      },
      "id": "kizWyMnd1GGn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5x_SCTo0jQ"
      },
      "source": [
        "#### Pandas-based tables in Python"
      ],
      "id": "jS5x_SCTo0jQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Pandas is the most widely used package for working with tabular data in Python. The term 'Panel Data' inspired the name. Pandas is a large, actively-developed package that is used by data analysts worldwide. It is licensed under the 3-clause BSD, like the rest of the scientific Python stack. Like any library, Pandas has certain advantages and disadvantages:\n"
      ],
      "metadata": {
        "id": "5jsgPTl0RNR1"
      },
      "id": "5jsgPTl0RNR1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Advantages include: \n",
        "\n",
        "* It has many useful built-in features that make data manipulation quick and easy \n",
        "* Many of Pandas’ features are optimized in C, making the library far faster than pure Python code\n",
        "* Because of its popularity, many other packages have ‘adaptors’ for getting data in and out of Pandas, and Pandas has input/output methods to read and write most common formats\n"
      ],
      "metadata": {
        "id": "4bf9WPi1RZGZ"
      },
      "id": "4bf9WPi1RZGZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Disadvantages include:"
      ],
      "metadata": {
        "id": "X5ADSUPsRqxr"
      },
      "id": "X5ADSUPsRqxr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pandas has some inconsistencies in its application programming interface (API) patterns, mainly because the library grew organically\n",
        "* As an open-source project, Pandas is continually improving. While backward compatibility is highly valued throughout the scientific Python ecosystem, breaking changes happen occasionally. This is why it is critical to always note the version of Pandas and other Python packages in publications for the purpose of reproducibility, and pay attention to warnings and release notes. Pandas deviates from the Pythonic ideal of “PEP 20 -- The Zen of Python” and offers multiple different ways to accomplish some operations, such as indexing. These alternatives may create challenges for new (or even experienced) users, but Pandas remains a powerful tool for versatile tabular computing operations, including workflows incorporating applications beyond Pandas (e.g. SQL, R, Spark). Regardless, the reader is encouraged to read through the Pandas documentation (which is excellent!) and become familiar with the various inputs and outputs of each available function.\n"
      ],
      "metadata": {
        "id": "A-a7zP58SL0j"
      },
      "id": "A-a7zP58SL0j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin using Pandas, import the library as follows:"
      ],
      "metadata": {
        "id": "6UGDYIo8SYb7"
      },
      "id": "6UGDYIo8SYb7"
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import pandas as pd"
      ],
      "metadata": {
        "id": "x938VOR3ShR0"
      },
      "id": "x938VOR3ShR0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models for working on tabular data**\n",
        "\n",
        "Splitting training and validation sets, applying many data augmentation techniques, creating the valuable data structures for importing data into the model, memory management by using appropriately sized mini-batches of data, and so on, can be time-consuming.\n",
        "\n",
        "**The following are some of the important arguments for this function:**\n",
        "\n",
        "**Input features:** It accepts data in the form of a Data Frame with spatial capabilities.\n",
        "\n",
        "**Variable predict:** Using simple logic, it automatically determines whether contents are continuous or categorical.\n",
        "\n",
        "**Explanatory variables:** List of containing explanatory variables from the input layer, where categorical variables require tuples.\n",
        "\n",
        "**Distance features:** Distance features are used to calculate the distance between provided features and input features to create explanatory variables automatically. Distances between the input explanatory distance features and the nearest Input training features will be calculated.\n",
        "\n",
        "\n",
        "**Data = prepare tabulardata (input features, variable predict, explanatory_variables)**\n",
        "\n"
      ],
      "metadata": {
        "id": "xQCHECmzSppO"
      },
      "id": "xQCHECmzSppO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0613f8d5-7d5b-48d0-acd6-8ac9e59a0d9a"
      },
      "source": [
        "### One-Hot Encoding\n",
        "\n",
        "\n"
      ],
      "id": "0613f8d5-7d5b-48d0-acd6-8ac9e59a0d9a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####One method of converting data to prepare it for an algorithm and improve prediction is one-hot encoding. One-Hot encoding is an important part of machine learning feature engineering. One-Hot encoding converts categorical data variables into variables that machine learning algorithms can use to improve predictions. Each value a categorical column can take becomes its own numerical column and is given a binary value of 1 or 0. For instance, if we have a categorical column for eye color with values of “Blue”, “Brown”, and “Green”, we would create 3 new columns (perhaps called “BlueEye”, “BrownEye”, and “GreenEye”) and set the value to ‘1’ in the ‘BlueEye’ column for those where the categorical value was ‘Blue’. Note that only one of these new columns may have a value of ‘1’, and the others must have a value of ‘0’.\n"
      ],
      "metadata": {
        "id": "rCICVYQoTVue"
      },
      "id": "rCICVYQoTVue"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b67fc951-d4fa-4a2b-a036-bc8ee974652a"
      },
      "source": [
        "### Where is it necessary to use one-hot encoding?"
      ],
      "id": "b67fc951-d4fa-4a2b-a036-bc8ee974652a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####A common application of One-Hot encoding is when categories have no relation to each other. In our example, GreenEyes are not ‘Greater Than’ BlueEyes or Browneyes, and GreenEyes are not closer to BrownEyes than BlueEyes. This is a case where One-Hot encoding is likely to improve performance, but in our example, GreenEyes are not 3 times more than BlueEyes. Numerical values should be used when the quantity has meaning. \n"
      ],
      "metadata": {
        "id": "3a6aOv6lTieW"
      },
      "id": "3a6aOv6lTieW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71a7990d-8a26-40d8-9441-11a121b400e2"
      },
      "source": [
        "### Advantages and disadvantages of One-hot encoding\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "71a7990d-8a26-40d8-9441-11a121b400e2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The advantage of One-Hot encoding is that the result is binary rather than ordinal, and everything is stored in an orthogonal vector space. The disadvantage is that the feature space can quickly expand for high cardinality (when a feature has many unique values), and we'll be fighting the curse of dimensionality. We usually use One-Hot encoding followed by a dimensional reduction technique such as Principal Component Analysis (PCA) because PCA looks for linear overlap, and will naturally group similar features together.  Other encoding schemes rarely beat the combination of One-Hot encoding and PCA."
      ],
      "metadata": {
        "id": "wUQlYJlgTwI3"
      },
      "id": "wUQlYJlgTwI3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###References:\n",
        "1. J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.\n",
        " \n",
        "2. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n",
        " \n",
        "3. Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A Generative Model for Raw Audio. In Proc. 9th ISCA Workshop on Speech Synthesis Workshop (SSW 9), page 125, 2016.\n",
        " \n",
        "4. Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794, 2016.\n",
        " \n",
        "5. Ravid Shwartz-Ziv, Amichai Painsky, and Naftali Tishby. Representation compression and generalization in deep neural networks. 2018.\n",
        " \n",
        " \n",
        "6. Tomaso Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks. Proceedings of the National Academy of Sciences, 117(48):30039–30045, 2020. ISSN 0027-8424. doi:10.1073/pnas.1907369117.\n",
        " \n",
        " \n",
        "7. Zoe Piran, Ravid Shwartz-Ziv, and Naftali Tishby. The dual information bottleneck. arXiv preprint arXiv:2006.04641, 2020.\n",
        " \n",
        "8. David HWolpert andWilliam G Macready. No free lunch theorems for optimization. IEEE transactions on evolutionary computation, 1(1):67–82, 1997.\n",
        " \n",
        "9. Sercan Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8):6679–6687, May 2021.\n",
        "\n",
        "10. Liran Katzir, Gal Elidan, and Ran El-Yaniv. Net-dnf: Effective deep modeling of tabular data. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\n",
        " \n",
        "11. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\n",
        " \n",
        "12. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:10.18653/v1/W18-5446.\n",
        " \n",
        "13. Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning on tabular data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n",
        " \n",
        "14. Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The tree ensemble layer: Differentiability meets conditional computation. In International Conference on Machine Learning, pages 4138–4148. PMLR, 2020.\n",
        " \n",
        "15. Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and Tom Goldstein. Saint: Improved neural networks for tabular data via row attention and contrastive pre-training. arXiv preprint arXiv:2106.01342, 2021.\n",
        " \n",
        "16. Baosenguo. baosenguo/kaggle-moa-2nd-place-solution, 2021. URL https://github.com/baosenguo/Kaggle-MoA-2nd-Place-Solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "dIgmrQtpUHlp"
      },
      "id": "dIgmrQtpUHlp"
    }
  ]
}