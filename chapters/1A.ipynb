{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Section 1: Introduction to Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mayo-Radiology-Informatics-Lab/MIDeL/blob/main/chapters/1A.ipynb)"
      ],
      "metadata": {
        "id": "73hWhcdtrqSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Author: Sanaz Vahdati, MD*"
      ],
      "metadata": {
        "id": "9CHQjamTrPI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section1A: Introduction to Machine Learning**"
      ],
      "metadata": {
        "id": "Y4AsdY9zsL7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This content is divided into three parts for Beginner, Intermediate and Advanced readers. Each part has a step-by-step learning guide, so readers with different programming backgrounds get the most use from it. Readers are expected to have a basic understanding of the Python programming language before beginning the present book (such as defining variables, data types, defining functions, object-oriented programming, and classes).\n",
        "\n",
        "In recent years, the application of artificial intelligence in medical imaging has grown significantly. People have long dreamed of using machine learning and deep learning to maximize the ability to recognize the disease in medical images. Consequently, learning and practicing these tools nowadays has become more valuable than ever. \n",
        "\n",
        "In this series, we have tried to give a better understanding of deep learning in the medical field by visualizing the 'concept' of how deep learning is used to diagnose and classify diseases from medical images (radiographic, CT, MR, ultrasound, and the various types of visible light images) by going through every detail of the process referring to different chapters. Our goal is to cover the beginner, intermediate and advanced levels of deep learning using PyTorch and Python. We will do this by combining traditional written material intermixed with Jupyter/Google Colab notebooks that will allow you to run (and alter!) real code that implements the described concepts. We believe having actual code can both help to reinforce the concepts and provide a ‘starter set’ for you to write your own applications.\n",
        "\n",
        "**About the code**:  All of the code in this book was written for Python 3.6 or later. The code for the book is available for download from the **MIDeL website** and on **GitHub**.\n",
        "\n",
        "**Hardware and Software Requirements**: To achieve results in a reasonable amount of time (which can still be hours), it is currently necessary to have an NVIDIA graphics processing unit (GPU) to accelerate the deep learning computations. Nowadays, most deep learning libraries can only utilize the power of NVIDIA GPUs.There is a special library provided by NVIDIA called CUDA that enables calculations to be performed on the GPU. Therefore, you will need access to an NVIDIA GPU and have the appropriate CUDA libraries installed. There are cloud-based providers of GPU computation that are free or inexpensive. One free option is provided by Google: http://colab.research.google.com. While the GPUs are not state-of-the-art, they are sufficient for at least demonstrating the concepts, and Colab will be the default computing environment. Colab also provides free access to tensor processing units (TPUs), which are accelerators that are specifically designed and optimized for deep learning computations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uOQC6gT_lqfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Artificial Intelligence?**\n",
        "\n",
        "Artificial intelligence (AI) is a branch of computer science with the goal of developing computer processes able to assess day-to-day problems that would normally require human input. Compared to traditional programs, where the algorithm directly solves the problem, AI requires both algorithms and data to develop a solution. In that sense, AI is data-driven solution development.\n"
      ],
      "metadata": {
        "id": "6FhtKsN-Q2KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Machine Learning?**\n",
        "\n",
        "Machine learning is a type of artificial intelligence in which a model learns patterns from provided data and gradually improves its performance. It typically requires the input data (that is, data similar to the data that will be analyzed once the model is trained) and the ‘answer’ (the prediction that the model should make for each example input). Assuming there is enough information in the input data to predict the answer and given enough examples, the machine learning model can make reasonably accurate predictions without being directly programmed.\n"
      ],
      "metadata": {
        "id": "kQtzVokyP1xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**What is deep learning, and what is its significance?**\n",
        "\n",
        "\n",
        "Deep learning is a type of machine learning that uses multi-layer networks (in addition to the input and output layers) to make a prediction from input data.  In deep learning, neural networks are used as a mathematical framework to learn and improve models’ performance. A neural network is created from at least three layers (input layer, at least one hidden layer, and an output layer). In its most basic form, deep learning is the development of neural networks with more than one hidden layer. The term neural network derives from neurobiology and represents an attempt to emulate the human brain's function.\n",
        "\n",
        "Deep learning has now proven valuable in many domains such as computer vision, cyber security, object detection, image processing, speech recognition, bioinformatics, medical imaging, medical information processing, and robotics  <a href='https://arxiv.org/abs/1802.06955'>(source)</a>\n",
        "\n",
        "We are surrounded by deep learning, performing tasks such as optimizing the pictures on our mobile phones, performing a forecast study of customer behavior, or detecting fraud in credit card transactions. Deep learning enables us to generate insights from massive volumes of data and is a key breakthrough in science, medicine, and various other fields. Deep learning has gained increasing interest in medical imaging and has been proven to produce state-of-the-art results in different applications and automation of many complex tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "VWdEsJn6yDPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **History of Deep Learning:**\n",
        "\n",
        "Deep learning may be dated back to 1943, when McCulloch & Pitts introduced the mathematical concept of a single-layer neural network. In 1950 Alan Turing described the \"Turing Test\" for assessing whether a machine is \"intelligent\" in his book entitled “Computing Machinery and Intelligence”. ​​The perceptron learning algorithm, the simplest type of neural network with only one layer of neuron and with actual learning capabilities to conduct binary classification, was invented by Frank Rosenplatt in 1958. In 1965, Ivakhnenko and Lapa proposed the multi-layer perceptron known as the first deep learning like algorithm.\n",
        "\n",
        "The term \"Deep Learning\" was first coined by Rina Detcher in 1986, and it has since become a word in the artificial intelligence world.\n",
        "By the late 1980s, Yann LeCun applied an optimized deep neural network to identify handwritten ZIP codes on mail, <a href='http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf'>(source)</a>.\n",
        "In 1990, Yann LeCun introduced LeNet, which demonstrated the practical application of deep neural networks. By 1991, such systems were being used to recognize isolated two-dimensional (2D) handwritten digits, and three-dimensional (3D) objects were being recognized by matching 2D images with a created 3D object model. \n",
        "While the idea of having multiple neural networks has existed since the 1950s, it has gained a lot of popularity since the early 2010s after beating all the existing algorithms in image classification in the ImageNet competition by a considerable margin. This large temporal gap between theory and adoption called “AI winter“ is attributed to the computational limitations and data access that were gradually improved in the 2010s.\n",
        "\n",
        "Below you can find a brief list of historical events during deep learning development:\n",
        "\n",
        "1943 (The first mathematical model of a neural network) - Walter Pitts and Warren McCulloch\n",
        "\n",
        "1950 (The prediction of machine learning) - Alan Turing\n",
        "\n",
        "1952 (First machine learning programs) - Arthur Samuel\n",
        "\n",
        "1957 (Setting the foundation for deep neural networks) - Frank Rosenblatt\n",
        "\n",
        "1959 (Discovery of simple cells and complex cells) - David H. Hubel and Torsten Wiesel\n",
        "\n",
        "1960 (Control theory) - Henry J. Kelley\n",
        "\n",
        "1965 (The first working deep learning networks) - Alexey Ivakhnenko and V.G. Lapa\n",
        "\n",
        "1979 (An ANN learns how to recognize visual patterns) - Kunihiko Fukushima\n",
        "\n",
        "1982 (The creation of the Hopfield Network) - John Hopfield\n",
        "\n",
        "1985 (A program learns to pronounce English words) - Terry Sejnowski\n",
        "\n",
        "1986 (Improvements in shape recognition and word prediction) - David Rumelhart, Geoffrey Hinton, and Ronald J. Williams\n",
        "\n",
        "1989 (Machines read handwritten digits) - Yann LeCun\n",
        "\n",
        "1989 (Q-learning) - Christopher Watkins\n",
        "\n",
        "1993 (A ‘very deep learning’ task is solved) - Jürgen Schmidhuber\n",
        "\n",
        "1995 (Support vector machines) - Corinna Cortes and Vladimir Vapnik\n",
        "\n",
        "1997 (Long short-term memory was proposed) - Jürgen Schmidhuber and Sepp Hochreiter\n",
        "\n",
        "1998 (Gradient-based learning) - Yann LeCun\n",
        "\n",
        "2009 (Launch of ImageNet) - Fei-Fei Li\n",
        "\n",
        "2011 (Creation of AlexNet) - Alex Krizhevsky\n",
        "\n",
        "2014 (DeepFace) - Yaniv Taigman\n",
        "\n",
        "2014 (Generative Adversarial Networks (GAN)) - Yann LeCun & Ian J. Goodfellow\n",
        "\n",
        "2016 (Powerful machine learning products)\n",
        "\n",
        "<br><br>\n",
        "Figure(1) provides a brief glimpse of the development of deep learning models over time. It spans the origins of neural networks to the models that have dominated deep learning research in the previous decade, such as convolutional neural networks, deep belief networks, and recurrent neural networks. <a href='https://www.kdnuggets.com/2018/03/weird-introduction-deep-learning.html#.Xh6pB-_AarA.twitter'>(source)</a>\n",
        " <br><br>\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/YyBfzzr/Screen-Shot-2022-06-01-at-11-32-20-AM.png\"><br>\n",
        "*Figure 1.* Timeline of major events in the history of deep learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "QDxNnQEEwrmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Major Steps in Creating a Deep Learning System**\n",
        "\n",
        "\n",
        "####**1. Define the Architecture** \n",
        "\n",
        "Deep learning models in each project go through a series of stages. The first step in developing a deep learning model is designing the architecture of the model in such a manner that it solves the issue successfully. We choose the type of model-building structure for the complete deep learning architecture in this step.\n",
        "\n",
        "The model’s architecture should be chosen based on the task at hand. Convolutional Neural Networks (CNNs), or ConvNets are widely used for image classification and recognition <a href='https://link.springer.com/article/10.1007/s10462-021-10039-7'>(source)</a>, for image segmentation tasks, U-Nets have shown to be highly successful <a href='https://arxiv.org/abs/1505.04597'>(source)</a>, and Recurrent Neural Networks (RNNs) have been the preferred structure for natural language processing (NLP) <a href='https://arxiv.org/abs/1505.04597'>(source)</a>.\n",
        "\n",
        "####**2. Compiling the Model**\n",
        "\n",
        "Now that we have developed the preferred architecture, we will move on to the following phase of compiling the model. Deep learning requires the compilation stage to build the model for the training process.Some of the key components of the training approach are established for the evaluation phase during the compilation process.\n",
        " \n",
        "You will learn in subsequent sections about the many choices you must make in designing a deep learning model. A few of these include the loss, optimizer, and error metrics. Briefly, the ‘loss’ is the summed error for the group of examples used for training. An optimizer adjusts the weights in a network to reduce the loss (and thus make a closer guess for future examples). The optimizers for computations are commonly Adam, RMSprop, or similar optimizers, and the metrics for analysis can be accuracy or any other user-defined statistic <a href='https://www.researchgate.net/profile/Farah-Alkhaled-2/publication/348590381_The_effect_of_optimizers_in_fingerprint_classification_model_utilizing_deep_learning/links/600673ad45851553a053e7f3/The-effect-of-optimizers-in-fingerprint-classification-model-utilizing-deep-learning.pdf'>(source)</a>. Error metrics will be similar to the loss but are expressed in more familiar terms. For instance, a classifier might have accuracy (the percent of correct classifications out of all examples) as an error metric but the loss will be the actual mathematical output of the network, which would also reflect the magnitude of error for each example.\n",
        "\n",
        "####**3. Fitting the Model**\n",
        "\n",
        "In the fitting phase, also known as training the model, we fit the model to the training dataset. With this function, we train the model with a defined number of epochs. While the training operation is in progress, the fitting performance must be assessed regularly (usually after each epoch). By monitoring the fitting process, we can often recognize when either the model is simply failing to learn or when the system begins to learn the specific examples in the data set and thus will not perform well on real-world examples, not in the training set. Once the training is completed and assessed for a predetermined number of epochs, we may go on to the following evaluation step and generate predictions with the trained model <a href='https://books.google.com/books?hl=en&lr=&id=C6qqCAAAQBAJ&oi=fnd&pg=PR5&ots=IrfjKprLvA&sig=yWh4_jjnCXQzdeMfOUKArToY2do#v=onepage&q&f=false'>(source)</a> .\n",
        "\n",
        "####**4. Evaluation and Prediction** \n",
        "\n",
        "Evaluating your deep learning model is critical in determining whether the model is performing as expected. There is a risk that the developed deep learning model will fail in real-world circumstances. When validating the trained model's effectiveness, one of the most popular techniques is to ‘hold out’ a portion of the whole dataset for evaluation of the model. In fact, sometimes, two such ‘holdouts’ are employed. There will always be a ‘training set,’ which is what the model uses to optimize its parameters. There is almost always a second set, variably referred to as either the ‘validation’ or ‘test’ set. The performance of the model on this data is done after each epoch, and this performance should reflect how the model would perform in the real world. As an extra check to assure expected performance, a third set (also called ‘validation’ or ‘test’ but the converse of what is used as the second set) is used as an extra check when training is entirely completed. In general, training should not be resumed based on performance on this 3rd evaluation set <a href='https://www.sciencedirect.com/science/article/pii/S0888327017306064'>(source)</a>.\n",
        "\n",
        "####**5. Deploying the Model**\n",
        "\n",
        "Deploying a model means applying it in the real world. If the purpose of developing a model was to prove a point and perhaps publish a paper, this deployment step is obviously not pursued. However, if a model is to be deployed, particularly in the medical imaging space, there are several considerations. The first is the technical aspect--how do you get data to the model, how do you execute it in a timely fashion, and how do you get results back.\n",
        "\n",
        "There are several options for addressing the computational needs, such as Amazon Web Service (AWS) or Google Cloud. However, for many medical applications, the computational load of making the prediction for one example (often called ‘inference’) is relatively small. In that case, the effort of transferring data to the cloud may not justify the benefits. And while a GPU is required for training, it is often not necessary for inference because of the low computational demand. Hospitals are also very security conscious, and transferring data outside the hospital firewall may not be possible.\n",
        "\n",
        "One must also be aware that if there are transformations done to the data prior to training, those same transformations must be applied for inference; else the data will be very different from what the model expects. Similarly, the output of a model is likely to be a number, but most medical use cases need that to be translated to something more meaningful to healthcare providers. An understanding of the relevant healthcare standards and how to do these transformations are described in later sections.\n",
        "\n",
        "In cases where direct-to-consumer applications are considered, deploying via web frameworks or directly onto mobile devices may be required.\n"
      ],
      "metadata": {
        "id": "jF6t4kNnTyi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Common Misconceptions about Deep Learning:**\n",
        "\n",
        "####**AI and Machine Learning are the same things.**\n",
        "Although they appear to be the same, they are as dissimilar as milk and butter. In a nutshell, artificial intelligence refers to a system’s ability to perform tasks in a way that we would deem\n",
        "‘intelligent,’ while machine learning is a current technique for creating AI. Machine learning is based on the concept of providing data to machines and allowing them to learn on their own. \n",
        "\n",
        "\n",
        "\n",
        "####**Machine learning is a magical process.**\n",
        "\n",
        "AI and machine learning, frequently depicted as \"magic\" in popular culture, are not new ideas. ML is not magic. It utilizes mathematics and statistics to learn from given data and then uses the learned knowledge to make predictions. ‘Interpretability’ of a model is the extent to which a model can be comprehended in human terms. The more a model is interpretable, the easier for a human to understand how the model predicts.\n",
        "Data analysis, pattern recognition, and iteration are all part of machine learning. The user can make a rookie AI by feeding it enough data to 'train' it, then incorporating a machine learning algorithm.\n",
        "\n",
        "####**Machine learning models develop over time**\n",
        "Another common misconception is that all we have to do is start the process, and the accuracy of the results will improve over time without any human involvement. However, it is crucial to keep in mind that machine learning models must be maintained regularly with various techniques such as hyperparameter optimization in order to enhance their performance and that this process will not happen without human intervention.\n",
        "\n",
        "\n",
        "####**The goal of Machine Learning Models is to give high accuracy**\n",
        "\n",
        "The evaluation of machine learning models has long been a source of discussion among experts. Machine learning models are not always designed to minimize the error or loss of the model, which is a common misperception. In addition, accuracy is not always a good indicator of a model's quality. \n",
        "One of the challenges in medical imaging is ‘data imbalance,’ which is mostly due to the different prevalence of diseases and various sources for data. For example, the prevalence of glioma is 6 in 100,000 in the general population which makes it a very rare disease. When a model is trained to detect this rare disease, if it always predicts new cases as normal it will have a very high accuracy while it is mostly missing the class of interest(glioma). In general, when a model is trained based on imbalanced data, it may poorly predict the minority class and will perform well on the majority class on which it is trained; thus overall its accuracy will be high. In this situation, some other predictive parameters such as Sensitivity (proportion of actual positive cases), Specificity (proportion of actual negative cases), or F score are used to evaluate the performance of the model.\n",
        "\n",
        "\n",
        "\n",
        "####**Machine Learning is all about prediction**\n",
        "\n",
        "Most people say they want machine learning to help them become more prescriptive. But that's not how it works. The truth is that ML can only be effective at forecasting the future if the future resembles previous trends <a href='https://www.frontiersin.org/articles/10.3389/frai.2020.524339/full'>(source)</a>.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cggAWItBL5rq"
      }
    }
  ]
}